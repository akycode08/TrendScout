"""
AI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç—Ä–µ–Ω–¥–æ–≤ —á–µ—Ä–µ–∑ Claude API

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Anthropic Claude API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç—Ä–µ–Ω–¥–∞—Ö:
- –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞/–Ω–∞–ø–∏—Ç–∫–∞
- –ö–∞—Ç–µ–≥–æ—Ä–∏—è
- –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ (sentiment)
- –í–∏—Ä—É—Å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª
- –ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∫ –±–∏–∑–Ω–µ—Å—É
"""

import json
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime

from anthropic import Anthropic
from anthropic import APIError, RateLimitError

from config import get_settings
from admin.usage_tracker import get_usage_tracker


# –ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤ (–∏–∑ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏)
ANALYSIS_PROMPT = """
Analyze this trend data to extract food/beverage trends.

TREND DATA:
Platform: {platform}
Content: {content}
Engagement: {views} views, {likes} likes, {comments} comments, {shares} shares
Posted: {posted_at}
Interest Score: {interest_score} (if available)

TASKS:
1. Identify the main food/beverage item mentioned (if any)
2. Categorize it (drink, pastry, main dish, snack, ingredient, etc.)
3. Assess sentiment (positive/negative/neutral)
4. Rate viral potential (0-10) based on engagement and interest
5. Determine if applicable to coffee shops/restaurants

IMPORTANT:
- If this is NOT a food/beverage trend, set restaurant_applicable to false
- Be specific with item names (e.g., "Lavender Oat Milk Latte" not just "latte")
- Consider current trends and popularity

Respond ONLY in valid JSON format:
{{
  "item_name": "exact name of the food/beverage item or null",
  "category": "drink|pastry|main_dish|snack|ingredient|other|null",
  "sentiment": "positive|negative|neutral",
  "viral_potential": 0-10,
  "restaurant_applicable": true|false,
  "reasoning": "brief explanation of your analysis"
}}
"""


class AIAnalyzer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤ —á–µ—Ä–µ–∑ Claude API
    
    –í—Å–µ –º–µ—Ç–æ–¥—ã —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ - –≤—ã–∑—ã–≤–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é: AIAnalyzer.analyze_batch(...)
    """
    
    # –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Claude API
    _client: Optional[Anthropic] = None
    
    @classmethod
    def _get_client(cls) -> Anthropic:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫–ª–∏–µ–Ω—Ç Claude API (singleton pattern)
        
        Returns:
            Anthropic: –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å API
        """
        if cls._client is None:
            settings = get_settings()
            cls._client = Anthropic(api_key=settings.anthropic_api_key)
        return cls._client
    
    @classmethod
    async def analyze_batch(
        cls, 
        data: List[Dict[str, Any]], 
        batch_size: int = 10,
        max_retries: int = 3
    ) -> List[Dict[str, Any]]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Claude API
        
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø–∞–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.
        –í–∫–ª—é—á–∞–µ—Ç retry –ª–æ–≥–∏–∫—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ API.
        
        Args:
            data: –°–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (—Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞ —Ä–∞–∑)
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            
        Returns:
            List[Dict]: –î–∞–Ω–Ω—ã–µ —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º AI –∞–Ω–∞–ª–∏–∑–æ–º
        """
        if not data:
            return []
        
        print(f"ü§ñ AI –∞–Ω–∞–ª–∏–∑ {len(data)} –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ Claude API...")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
        batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]
        analyzed_data = []
        
        for batch_num, batch in enumerate(batches, 1):
            print(f"   –ë–∞—Ç—á {batch_num}/{len(batches)} ({len(batch)} –ø–æ—Å—Ç–æ–≤)...")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –ø–æ—Å—Ç –≤ –±–∞—Ç—á–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            tasks = [cls._analyze_single(item, max_retries) for item in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for item, result in zip(batch, results):
                if isinstance(result, Exception):
                    print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å—Ç–∞ {item.get('post_id', 'unknown')}: {result}")
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±–µ–∑ AI –∞–Ω–∞–ª–∏–∑–∞
                    item['ai_analysis'] = None
                    item['ai_error'] = str(result)
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã AI –∞–Ω–∞–ª–∏–∑–∞
                    item['ai_analysis'] = result
                    analyzed_data.append(item)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å rate limit
            if batch_num < len(batches):
                await asyncio.sleep(1)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
        relevant_trends = [
            item for item in analyzed_data 
            if item.get('ai_analysis', {}).get('restaurant_applicable', False)
        ]
        
        print(f"‚úÖ AI –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(relevant_trends)}/{len(data)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤")
        
        return analyzed_data
    
    @classmethod
    async def _analyze_single(
        cls, 
        item: Dict[str, Any], 
        max_retries: int = 3
    ) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–¥–∏–Ω –ø–æ—Å—Ç —á–µ—Ä–µ–∑ Claude API
        
        Args:
            item: –î–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç–∞
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã AI –∞–Ω–∞–ª–∏–∑–∞
        """
        client = cls._get_client()
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ—Å—Ç–∞
        prompt = ANALYSIS_PROMPT.format(
            platform=item.get('platform', 'unknown'),
            content=item.get('content', '')[:500],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            views=item.get('views', 0),
            likes=item.get('likes', 0),
            comments=item.get('comments', 0),
            shares=item.get('shares', 0),
            posted_at=str(item.get('posted_at', 'unknown')),
            interest_score=item.get('interest_score', 'N/A')
        )
        
        # –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å —Å retry –ª–æ–≥–∏–∫–æ–π
        for attempt in range(max_retries):
            try:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Claude API
                # Anthropic SDK —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º run_in_executor
                loop = asyncio.get_event_loop()
                message = await loop.run_in_executor(
                    None,
                    lambda: client.messages.create(
                        model="claude-sonnet-4-20250514",  # –ü–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è Claude Sonnet
                        max_tokens=1000,
                        system="You are a food and beverage trend analyst. Analyze trends and provide structured JSON responses.",
                        messages=[{
                            "role": "user",
                            "content": prompt
                        }]
                    )
                )
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
                response_text = message.content[0].text
                
                # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Claude API
                tracker = get_usage_tracker()
                input_tokens = message.usage.input_tokens
                output_tokens = message.usage.output_tokens
                tracker.track_claude_request(
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    model="claude-sonnet-4-20250514"
                )
                
                # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
                try:
                    # –ò–Ω–æ–≥–¥–∞ Claude –¥–æ–±–∞–≤–ª—è–µ—Ç markdown —Ä–∞–∑–º–µ—Ç–∫—É, —É–±–∏—Ä–∞–µ–º –µ—ë
                    if "```json" in response_text:
                        response_text = response_text.split("```json")[1].split("```")[0].strip()
                    elif "```" in response_text:
                        response_text = response_text.split("```")[1].split("```")[0].strip()
                    
                    analysis = json.loads(response_text)
                    
                    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–∞
                    required_fields = ['item_name', 'category', 'sentiment', 'viral_potential', 'restaurant_applicable']
                    if all(field in analysis for field in required_fields):
                        return analysis
                    else:
                        raise ValueError(f"Missing required fields in AI response: {analysis}")
                        
                except json.JSONDecodeError as e:
                    print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
                    print(f"   –û—Ç–≤–µ—Ç Claude: {response_text[:200]}...")
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    return {
                        "item_name": None,
                        "category": None,
                        "sentiment": "neutral",
                        "viral_potential": 5,
                        "restaurant_applicable": False,
                        "reasoning": f"JSON parse error: {e}"
                    }
                
            except RateLimitError:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    print(f"   ‚ö†Ô∏è  Rate limit, –∂–¥–µ–º {wait_time} —Å–µ–∫...")
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise
                    
            except APIError as e:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2
                    print(f"   ‚ö†Ô∏è  API –æ—à–∏–±–∫–∞, –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {wait_time} —Å–µ–∫: {e}")
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise
                    
            except Exception as e:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫ –Ω–µ –¥–µ–ª–∞–µ–º retry
                raise
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å
        raise Exception(f"Failed to analyze after {max_retries} attempts")
    
    @classmethod
    def extract_trends(cls, analyzed_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ—á—å —Ç—Ä–µ–Ω–¥—ã –∏–∑ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Å—Ç—ã –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ç—Ä–µ–Ω–¥–∞ –∏ —Å–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç—ã Trend.
        
        Args:
            analyzed_data: –î–∞–Ω–Ω—ã–µ —Å AI –∞–Ω–∞–ª–∏–∑–æ–º
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤
        """
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
        relevant = [
            item for item in analyzed_data
            if item.get('ai_analysis', {}).get('restaurant_applicable', False)
            and item.get('ai_analysis', {}).get('item_name')
        ]
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ç—Ä–µ–Ω–¥–∞
        trends_dict = {}
        
        for item in relevant:
            analysis = item.get('ai_analysis', {})
            item_name = analysis.get('item_name')
            
            if item_name not in trends_dict:
                trends_dict[item_name] = {
                    'trend_name': item_name,
                    'category': analysis.get('category'),
                    'vertical': item.get('vertical', 'coffee'),
                    'sentiment': analysis.get('sentiment', 'neutral'),
                    'viral_potential': analysis.get('viral_potential', 0),
                    'ai_confidence': 0.8,  # –ú–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ—Å—Ç–æ–≤
                    'description': analysis.get('reasoning', ''),
                    'posts': [],
                    'platforms': set(),
                    'total_views': 0,
                    'total_engagement': 0
                }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å—Ç –∫ —Ç—Ä–µ–Ω–¥—É
            trends_dict[item_name]['posts'].append(item)
            trends_dict[item_name]['platforms'].add(item.get('platform'))
            trends_dict[item_name]['total_views'] += item.get('views', 0)
            trends_dict[item_name]['total_engagement'] += (
                item.get('likes', 0) + 
                item.get('comments', 0) * 3 + 
                item.get('shares', 0) * 5
            )
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º sets –≤ lists –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        trends = []
        for trend_data in trends_dict.values():
            trend_data['platforms'] = list(trend_data['platforms'])
            trends.append(trend_data)
        
        return trends

